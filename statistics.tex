% Define knitr options
% !Rnw weave=knitr
% Set global chunk options



% Define document options
\documentclass[10pt]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0, 0, 0}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.502,0,0.502}{\textbf{#1}}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.651,0.522,0}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{1,0.502,0}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0,0.502}{\textbf{#1}}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.733,0.475,0.467}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.502,0.502,0.753}{\textbf{#1}}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0,0.502,0.753}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0,0.267,0.4}{#1}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbold package for unitary vector or matrix symbol
\usepackage{bbold}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape,bg=red,fg=red}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[Probability and Statistics]{Probability and Statistics}
\subtitle{FRE6871 \& FRE7241, Spring 2016}
% \subject{Getting Started With R}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@poly.edu}
% \date{January 27, 2014}
\date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Probability and Statistics}


%%%%%%%%%%%%%%%
\subsection{Generating Pseudo-Random Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Random number generators produce the same deterministic sequence of numbers after their \texttt{seed} value is reset,
      \vskip1ex
      The function \texttt{set.seed()} initializes the random number generator by specifying the \texttt{seed} value,
      \vskip1ex
      The function \texttt{runif()} produces random numbers from the \emph{uniform} distribution,
      \vskip1ex
      The function \texttt{rnorm()} produces random numbers from the \emph{normal} distribution,
      \vskip1ex
      The function \texttt{pnorm()} calculates the cumulative \emph{normal} distribution,
      \vskip1ex
      The function \texttt{qnorm()} calculates the inverse cumulative \emph{normal} distribution,
    \column{0.6\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> set.seed(1121)  # reset random number generator
> runif(3)  # three random numbers from the uniform distribution
> runif(3)  # produce another three numbers
> set.seed(1121)  # reset random number generator
> runif(3)  # produce another three numbers
> 
> # produce random number from standard normal distribution
> rnorm(1)
> # produce five random numbers from standard normal distribution
> rnorm(5)
> # produce five random numbers from the normal distribution
> rnorm(n=5, mean=1, sd=2)  # match arguments by name
> # calculate cumulative standard normal distribution
> c(pnorm(-2), pnorm(2))
> # calculate inverse cumulative standard normal distribution
> c(qnorm(0.75), qnorm(0.25))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Binomial Random Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      A \emph{binomial} trial is a coin flip, that results in either a success or failure,
      \vskip1ex
      The \emph{binomial} distribution specifies the probability of obtaining a certain number of successes in a sequence of independent \emph{binomial} trials,
      \vskip1ex
      Let $p$ be the probability of obtaining a success in a \emph{binomial} trial, and let $(1-p)$ be the probability of failure,
      \vskip1ex
      $p = 0.5$ corresponds to flipping an unbiased coin,
      \vskip1ex
      The probability of obtaining $k$ successes in $n$ independent \emph{binomial} trials is equal to:
      \begin{displaymath}
        {n \choose k} p^k (1-p)^{(n-k)}
      \end{displaymath}
      The function \texttt{rbinom()} produces random numbers from the \emph{binomial} distribution,
    \column{0.6\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> set.seed(1121)  # reset random number generator
> # flip unbiased coin once, 20 times
> rbinom(n=20, size=1, 0.5)
> # number of heads after flipping twice, 20 times
> rbinom(n=20, size=2, 0.5)
> # number of heads after flipping thrice, 20 times
> rbinom(n=20, size=3, 0.5)
> # number of heads after flipping biased coin thrice, 20 times
> rbinom(n=20, size=3, 0.8)
> # number of heads after flipping biased coin thrice, 20 times
> rbinom(n=20, size=3, 0.2)
> # flip unbiased coin once, 20 times
> sample(x=0:1, size=20, replace=TRUE)  # fast
> as.numeric(runif(20) < 0.5)  # slower
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Random Samples and Permutations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{sample} is a subset of elements taken from a set of data elements,
      \vskip1ex
      The function \texttt{sample()} produces a random sample form a vector of data elements,
      \vskip1ex
      By default the \emph{size} of the sample (the \texttt{size} argument) is equal to the number of elements in the data vector,
      \vskip1ex
      So the call \texttt{sample(da\_ta)} produces a random permutation of all the elements of \texttt{da\_ta},
      \vskip1ex
      If \texttt{replace=TRUE}, then \texttt{sample()} produces samples with replacement,
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # permutation of five numbers
> sample(x=5)
> # permutation of four strings
> sample(x=c("apple", "grape", "orange", "peach"))
> # sample of size three
> sample(x=5, size=3)
> # sample with replacement
> sample(x=5, replace=TRUE)
> sample(  # sample of strings
+   x=c("apple", "grape", "orange", "peach"),
+   size=12,
+   replace=TRUE)
> # binomial sample: flip coin once, 20 times
> sample(x=0:1, size=20, replace=TRUE)
> # flip unbiased coin once, 20 times
> as.numeric(runif(20) > 0.5)  # slower
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Statistical Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A data \emph{sample} is a set of observations of a \emph{random variable},
      \vskip1ex
      Let $\{x_1,\ldots ,x_n\}$ be a data \emph{sample} of a \emph{random variable} \texttt{x},
      \vskip1ex
      Let \texttt{x} follow a probability distribution with population mean equal to $\mu$ and population standard deviation equal to $\sigma$,
      \vskip1ex
      A \emph{statistic} is a function of a data \emph{sample}:  $f( x_1,\ldots ,x_n )$,
      \vskip1ex
      A \emph{statistic} is itself a \emph{random variable},
      \vskip1ex
      A statistical \emph{estimator} is a \emph{statistic} that provides an estimate of a \emph{distribution} parameter,
      \vskip1ex
      For example:
      \begin{displaymath}
        \bar{x}=\frac{1}{n}{\sum_{i=1}^{n}x_i}
      \end{displaymath}
      Is an \emph{estimator} of the \emph{mean} of the \emph{distribution},
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # sample from Standard Normal Distribution
> sam_ple <- rnorm(1000)
> 
> mean(sam_ple)  # sample mean
> 
> median(sam_ple)  # sample median
> 
> sd(sam_ple)  # sample standard deviation
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimators of Higher Moments}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimators of moments of a probability distribution, based on a \emph{sample} of data, are given by:
      \vskip1ex
      Sample mean: $\hat\mu=\frac{1}{n} \sum_{i=1}^{n} x_i$
      \vskip1ex
      Sample variance: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^{n} (x_i-\hat\mu)^2$
      \vskip1ex
      With their expected values equal to the population mean and standard deviation:\\
      $\mathbb{E[\hat\mu]=\mu}$ \hskip0.5em and \hskip0.5em $\mathbb{E[\hat\sigma]=\sigma}$
      \vskip1ex
      The sample skewness (third moment) is equal to:
      \begin{displaymath}
        \hat{s}=\frac{n}{(n-1)(n-2)} \sum_{i=1}^{n} (\frac{x_i-\hat\mu}{\hat\sigma})^3
      \end{displaymath}
      The sample kurtosis (fourth moment) is equal to
      \begin{displaymath}
        \hat{k}=\frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum_{i=1}^{n} (\frac{x_i-\hat\mu}{\hat\sigma})^4
      \end{displaymath}
      The normal distribution has zero skewness and kurtosis equal to 3,
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # DAX returns
> ts_rets <- diff(log(EuStockMarkets[, 1]))
> # number of observations
> len_rets <- length(ts_rets)
> # mean of DAX returns
> mean_rets <- mean(ts_rets)
> # standard deviation of DAX returns
> sd_rets <- sd(ts_rets)
> # skew of DAX returns
> len_rets/((len_rets-1)*(len_rets-2))*
+   sum(((ts_rets - mean_rets)/sd_rets)^3)
> # kurtosis of DAX returns
> len_rets*(len_rets+1)/((len_rets-1)^3)*
+   sum(((ts_rets - mean_rets)/sd_rets)^4)
> # random normal returns
> ts_rets <- rnorm(len_rets, sd=2)
> # mean and standard deviation of random normal returns
> mean_rets <- mean(ts_rets)
> sd_rets <- sd(ts_rets)
> # skew of random normal returns
> len_rets/((len_rets-1)*(len_rets-2))*
+   sum(((ts_rets - mean_rets)/sd_rets)^3)
> # kurtosis of random normal returns
> len_rets*(len_rets+1)/((len_rets-1)^3)*
+   sum(((ts_rets - mean_rets)/sd_rets)^4)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Statistical estimators are themselves \emph{random variables},
      \vskip1ex
      The standard deviation of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_{\mu} = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the population standard deviation,
      \vskip1ex
      The standard error is the estimator of the standard deviation, and for the mean estimator it is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^{n} (x_i-\hat\mu)^2$ is the sample standard deviation (the estimator of the population standard deviation),
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # sample from Standard Normal Distribution
> sample_length <- 1000
> sam_ple <- rnorm(sample_length)
> # sample mean
> mean(sam_ple)
> # sample standard deviation
> sd(sam_ple)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Hypothesis Testing}


%%%%%%%%%%%%%%%
\subsection{Hypothesis Testing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Hypothesis Testing} is designed to test the validity of a \emph{null hypothesis},
      \vskip1ex
      A \emph{Hypothesis Test} consists of:
      \begin{itemize}
        \item \emph{null hypothesis},
        \item test \emph{statistic} (from sample),
        \item \emph{significance level} $\alpha$, determining whether to accept or reject the \emph{null hypothesis},
        \item \emph{p}-value (probability of observing the test statistic value, assuming the \emph{null hypothesis} is \texttt{TRUE}),
      \end{itemize}
      If the \emph{p}-value is less than the \emph{significance level} $\alpha$, then the \emph{null hypothesis} is rejected,
      \vskip1ex
      The objective of \emph{Hypothesis Testing} is to invalidate the \emph{null hypothesis},
      \vskip1ex
      In statistics we cannot \emph{prove} that a hypothesis is \texttt{TRUE}; we can only conclude that it's very unlikely to be \texttt{FALSE} (given the data),
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ### Perform two-tailed test that sample is
> ### from Standard Normal Distribution (mean=0, SD=1)
> # generate vector of samples and store in data frame
> test_frame <- data.frame(samples=rnorm(1000))
> 
> # significance level, two-tailed test, critical value=2*SD
> signif_level <- 2*(1-pnorm(2))
> signif_level
> # get p-values for all the samples
> test_frame$p_values <-
+   sapply(test_frame$samples, pnorm)
> test_frame$p_values <-
+   2*(0.5-abs(test_frame$p_values-0.5))
> # compare p_values to significance level
> test_frame$result <-
+   test_frame$p_values > signif_level
> # number of null rejections
> sum(!test_frame$result)
> # show null rejections
> head(test_frame[!test_frame$result, ])
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Visualizing Hypothesis Testing Using Package \texttt{ggplot2}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(ggplot2)  # load ggplot2
> 
> qplot(  # simple ggplot2
+     main="Standard Normal Distribution",
+     c(-4, 4),
+     stat="function",
+     fun=dnorm,
+     geom="line",
+     xlab=NULL, ylab=NULL
+     ) +  # end qplot
+ 
+ theme(  # modify plot theme
+     plot.title=element_text(vjust=-1.0),
+     plot.background=element_blank()
+     ) +  # end theme
+ 
+ geom_vline(  # add vertical line
+   aes(xintercept=c(-2.0, 2.0)),
+   colour="red",
+   linetype="dashed"
+   )  # end geom_vline
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/hyp_test_ggp2-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Visualizing Hypothesis Testing Using \texttt{ggplot2} (cont.)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ### create ggplot2 with shaded area
> x_var <- -400:400/100
> norm_frame <- data.frame(x_var=x_var,
+                  d.norm=dnorm(x_var))
> norm_frame$shade <- ifelse(
+             abs(norm_frame$x_var) >= 2,
+             norm_frame$d.norm, NA)
> ggplot(  # main function
+   data=norm_frame,
+   mapping=aes(x=x_var, y=d.norm)
+   ) +  # end ggplot
+ # plot line
+   geom_line() +
+ # plot shaded area
+   geom_ribbon(aes(ymin=0, ymax=shade), fill="red") +
+ # no axis labels
+   xlab("") + ylab("") +
+ # add title
+   ggtitle("Standard Normal Distribution") +
+ # modify plot theme
+   theme(
+   plot.title=element_text(vjust=-1.0),
+   plot.background=element_blank()
+   )  # end theme
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/hyp_test_ggp2_2-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\section{Univariate Statistical Analysis}


%%%%%%%%%%%%%%%
\subsection{Shapiro-Wilk Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Shapiro-Wilk} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1,\ldots ,x_n\}$ is from a normally distributed population,
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        W= \frac {(\sum_{i=1}^{n} a_i x_{(i)})^2} {\sum_{i=1}^{n} (x_i-\bar{x})^2}
      \end{displaymath}
      Where the: $\{a_1,\ldots ,a_n\}$ are proportional to the \emph{order statistics} of random variables from the normal distribution,
      \vskip1ex
      $x_{(k)}$ is the \emph{k}-th \emph{order statistic}, and is equal to the \emph{k}-th smallest value in the sample: $\{x_1,\ldots ,x_n\}$,
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic follows its own distribution, and is less than or equal to one,
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic is close to one for samples from normal distributions,
      \vskip1ex
      The \emph{p}-value for DAX returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the DAX returns are not from a normally distributed population,
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # calculate DAX percentage returns
> dax_rets <- diff(log(EuStockMarkets[, 1]))
> 
> # Shapiro-Wilk test for normal distribution
> shapiro.test(rnorm(length(dax_rets)))
> 
> # Shapiro-Wilk test for DAX returns
> shapiro.test(dax_rets)
> 
> # Shapiro-Wilk test for uniform distribution
> shapiro.test(runif(length(dax_rets)))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Jarque-Bera Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Jarque-Bera} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1,\ldots ,x_n\}$ is from a normally distributed population,
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        JB= \frac{n}{6} (\hat{s}^2 + \frac{1}{4} (\hat{k} - 3)^2)
      \end{displaymath}
      Where the skewness and kurtosis are defined as:
      \begin{align*}
        \hat{s} = \frac{1}{n} \sum_{i=1}^{n} (\frac{x_i-\bar{x}}{\hat\sigma})^3
      &&
        \hat{k} = \frac{1}{n} \sum_{i=1}^{n} (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{align*}
      The \emph{Jarque-Bera} statistic asymptotically follows the \emph{chi-squared} distribution with two degrees of freedom,
      \vskip1ex
      The \emph{Jarque-Bera} statistic is small for samples from normal distributions,
      \vskip1ex
      The \emph{p}-value for DAX returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the DAX returns are not from a normally distributed population,
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(tseries)  # load package tseries
> 
> # Jarque-Bera test for normal distribution
> jarque.bera.test(rnorm(length(dax_rets)))
> 
> # Jarque-Bera test for DAX returns
> jarque.bera.test(dax_rets)
> 
> # Jarque-Bera test for uniform distribution
> jarque.bera.test(runif(length(dax_rets)))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Autocorrelation Function} is the correlation coefficient of a time series with its lagged values:
      \begin{displaymath}
        \rho_k = \frac{1}{(n-k)\sigma^2} {\sum_{i=k+1}^{n} (x_i-\bar{x})(x_{i-k}-\bar{x})}
      \end{displaymath}
      \vskip1ex
      The function \texttt{acf()} from the base package \texttt{stats} calculates and plots the autocorrelation function for a univariate time series,
      \vskip1ex
      \texttt{acf()} returns the \texttt{acf} data invisibly - the return value isn't automatically printed to the console,
      \vskip1ex
      The \texttt{acf()} return data can be assigned to a variable, and then printed,
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(zoo)  # load package zoo
> # autocorrelation from "stats"
> acf(coredata(dax_rets), lag=10, main="")
> title(main="acf of DAX returns", line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
      The package \texttt{zoo} is designed for managing \emph{time series} and ordered objects,
      \vskip1ex
      \texttt{coredata} extracts the core underlying data from a complex object,
    \column{0.5\textwidth}
      \vspace{-1.0em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/acf_func-1}
      \vspace{-3.0em}
      The horizontal dashed lines are confidence intervals of the autocorrelation estimator (at 95\% significance level),
      \vskip1ex
      The DAX time series of returns does not appear to have statistically significant autocorrelations,
      \vskip1ex
      The function \texttt{acf()} has the drawback that it plots the lag-zero autocorrelation (which is simply \texttt{1}),
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Improved Autocorrelation Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Inspection of the data returned by \texttt{acf()} shows how to omit the lag-zero autocorrelation,
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> dax_acf <- acf(coredata(dax_rets), plot=FALSE)
> summary(dax_acf)  # get the structure of the "acf" object
> # print(dax_acf)  # print acf data
> dim(dax_acf$acf)
> dim(dax_acf$lag)
> head(dax_acf$acf)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      The below wrapper function for \texttt{acf()} omits the lag-zero autocorrelation,
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> acf_plus <- function (ts_data, plot=TRUE,
+                 xlab="Lag", ylab="",
+                 main="", ...) {
+   acf_data <- acf(x=ts_data, plot=FALSE, ...)
+ # remove first element of acf data
+   acf_data$acf <-  array(data=acf_data$acf[-1],
+     dim=c((dim(acf_data$acf)[1]-1), 1, 1))
+   acf_data$lag <-  array(data=acf_data$lag[-1],
+     dim=c((dim(acf_data$lag)[1]-1), 1, 1))
+   if (plot) {
+     ci <- qnorm((1+0.95)/2)*sqrt(1/length(ts_data))
+     ylim <- c(min(-ci, range(acf_data$acf[-1])),
+         max(ci, range(acf_data$acf[-1])))
+     plot(acf_data, xlab=xlab, ylab=ylab,
+    ylim=ylim, main=main, ci=0)
+     abline(h=c(-ci, ci), col="blue", lty=2)
+   }
+   invisible(acf_data)  # return invisibly
+ }  # end acf_plus
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of DAX Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The DAX time series of returns does not appear to have statistically significant autocorrelations,
      \vskip1ex
      But the \texttt{acf} plot alone is not enough to test whether autocorrelations are statistically significant or not,
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # improved autocorrelation function
> acf_plus(coredata(dax_rets), lag=10, main="")
> title(main="acf of DAX returns", line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/dax_acf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Squared DAX Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Squared DAX returns do have statistically significant autocorrelations,
      \vskip1ex
      But squared random returns are not autocorrelated,
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # autocorrelation of squared DAX returns
> acf_plus(coredata(dax_rets)^2,
+    lag=10, main="")
> title(main="acf of squared DAX returns",
+ line=-1)
> # autocorrelation of squared random returns
> acf_plus(rnorm(length(dax_rets))^2,
+    lag=10, main="")
> title(main="acf of squared random returns",
+ line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/dax_squared_acf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{U.S. Macroeconomic Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \texttt{Ecdat} contains the \texttt{Macrodat} U.S. macroeconomic data,
      \vskip1ex
      \texttt{"lhur"} is the unemployment rate (average of months in quarter),
      \vskip1ex
      \texttt{"fygm3"} 3 month treasury bill interest rate (last month in quarter)
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(Ecdat)  # load Ecdat
> colnames(Macrodat)  # United States Macroeconomic Time Series
> macro_zoo <- as.zoo(  # coerce to "zoo"
+     Macrodat[, c("lhur", "fygm3")])
> colnames(macro_zoo) <- c("unemprate", "3mTbill")
> # ggplot2 in multiple panes
> autoplot(  # generic ggplot2 for "zoo"
+   object=macro_zoo, main="US Macro",
+   facets=Series ~ .) + # end autoplot
+   xlab("") +
+ theme(  # modify plot theme
+   legend.position=c(0.1, 0.5),
+   plot.title=element_text(vjust=-2.0),
+   plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
+   plot.background=element_blank(),
+   axis.text.y=element_blank()
+ )  # end theme
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/macro_data-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Econometric Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most econometric data displays a high degree of autocorrelation,
      \vskip1ex
      But time series of tradeable prices display very low autocorrelation,
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> macro_diff <- na.omit(diff(macro_zoo))
> 
> acf_plus(coredata(macro_diff[, "unemprate"]),
+    lag=10)
> title(main="quarterly unemployment rate",
+ line=-1)
> 
> acf_plus(coredata(macro_diff[, "3mTbill"]),
+    lag=10)
> title(main="3 month T-bill EOQ", line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/macro_corr-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ljung-Box Test of Autocorrelation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ljung-Box} test \emph{null hypothesis} is that autocorrelations are equal to zero,
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        Q = n(n+2) \sum_{k=1}^{maxlag} \frac{{\hat\rho}_k^2}{n-k}
      \end{displaymath}
      Where $n$ is the sample size, and the ${\hat\rho}_k$ are sample autocorrelations,
      \vskip1ex
      The \emph{Ljung-Box} statistic follows the \emph{chi-squared} distribution with \emph{maxlag} degrees of freedom,
      \vskip1ex
      The \emph{Ljung-Box} statistic is small for time series that are \emph{not} autocorrelated,
      \vskip1ex
      The \emph{p}-value for DAX returns is large, and we conclude that the \emph{null hypothesis} is \texttt{TRUE}, and that DAX returns are \emph{not} autocorrelated,
      \vskip1ex
      The \emph{p}-value for changes in econometric data is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and that econometric data \emph{are} autocorrelated,
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Ljung-Box test for DAX data
> # 'lag' is the number of autocorrelation coefficients
> Box.test(dax_rets, lag=10, type="Ljung")
> 
> # changes in 3 month T-bill rate are autocorrelated
> Box.test(macro_diff[, "3mTbill"],
+    lag=10, type="Ljung")
> 
> # changes in unemployment rate are autocorrelated
> Box.test(macro_diff[, "unemprate"],
+    lag=10, type="Ljung")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Filtering Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(zoo)  # load zoo
> library(ggplot2)  # load ggplot2
> library(gridExtra)  # load gridExtra
> # extract DAX time series
> dax_ts <- EuStockMarkets[, 1]
> # filter past values only (sides=1)
> dax_filt <- filter(dax_ts,
+              filter=rep(1/5,5), sides=1)
> # coerce to zoo and merge the time series
> dax_filt <- merge(as.zoo(dax_ts),
+             as.zoo(dax_filt))
> colnames(dax_filt) <- c("DAX", "DAX filtered")
> dax_data <- window(dax_filt,
+              start=1997, end=1998)
> autoplot(  # plot ggplot2
+     dax_data, main="Filtered DAX",
+     facets=NULL) +  # end autoplot
+ xlab("") + ylab("") +
+ theme(  # modify plot theme
+     legend.position=c(0.1, 0.5),
+     plot.title=element_text(vjust=-2.0),
+     plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
+     plot.background=element_blank(),
+     axis.text.y=element_blank()
+     )  # end theme
> # end ggplot2
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/dax_filter-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation Function of Filtered Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filtering a time series creates autocorrelations,
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> dax_rets <- na.omit(diff(log(dax_filt)))
> par(mfrow=c(2,1))  # set plot panels
> 
> acf_plus(coredata(dax_rets[, 1]), lag=10,
+    xlab="")
> title(main="DAX", line=-1)
> 
> acf_plus(coredata(dax_rets[, 2]), lag=10,
+    xlab="")
> title(main="DAX filtered", line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/dax_filter_acf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Partial Autocorrelation Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{partial autocorrelation} of lag \texttt{k} is the autocorrelation after all the autocorrelations of lag \texttt{1,..., k-1} have been removed,
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> par(mfrow=c(2,1))  # set plot panels
> # autocorrelation from "stats"
> acf_plus(dax_rets[, 2], lag=10, xlab=NA, ylab=NA)
> title(main="DAX filtered autocorrelations", line=-1)
> # partial autocorrelation
> pacf(dax_rets[, 2], lag=10, xlab=NA, ylab=NA)
> title(main="DAX filtered partial autocorrelations",
+       line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/eustx_pacf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} time series process \emph{AR(p)} of order \emph{p} is defined as:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_{p} r_{i-p} + \varepsilon_i
      \end{displaymath}
      Where the $\varepsilon_i$ are independent random variables with zero mean and constant variance,
      \vskip1ex
      The \emph{AR(p)} process is a special case of an \emph{ARIMA} process,
      \vskip1ex
      The function \texttt{arima.sim()} simulates \emph{ARIMA} processes,
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> in_dex <- Sys.Date() + 0:728  # two year daily series
> set.seed(1121)  # reset random numbers
> zoo_arima <- zoo(  # AR time series of returns
+   x=arima.sim(n=729, model=list(ar=0.2)),
+   order.by=in_dex)  # zoo_arima
> zoo_arima <- cbind(zoo_arima, cumsum(zoo_arima))
> colnames(zoo_arima) <- c("AR returns", "AR prices")
> autoplot(object=zoo_arima, # ggplot AR process
+  facets="Series ~ .",
+  main="Autoregressive process (phi=0.2)") +
+   facet_grid("Series ~ .", scales="free_y") +
+   xlab("") + ylab("") +
+ theme(
+   legend.position=c(0.1, 0.5),
+   plot.background=element_blank(),
+   axis.text.y=element_blank())
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/ar_process-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Examples of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{"model"} argument contains a \texttt{list} of \emph{ARIMA} coefficients $\{\varphi_i\}$,
      \vskip1ex
      Positive coefficient values cause positive \emph{autocorrelation}, and vice cersa,
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ar_coeff <- c(-0.8, 0.01, 0.8)  # AR coefficients
> zoo_arima <- sapply(  # create three AR time series
+   ar_coeff, function(phi) {
+     set.seed(1121)  # reset random numbers
+     arima.sim(n=729, model=list(ar=phi))
+   } )
> zoo_arima <- zoo(x=zoo_arima, order.by=in_dex)
> # convert returns to prices
> zoo_arima <- cumsum(zoo_arima)
> colnames(zoo_arima) <-
+   paste("autocorr", ar_coeff)
> autoplot(zoo_arima, main="AR prices",
+    facets=Series ~ .) +
+     facet_grid(Series ~ ., scales="free_y") +
+ xlab("") +
+ theme(
+   legend.position=c(0.1, 0.5),
+   plot.title=element_text(vjust=-2.0),
+   plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
+   plot.background=element_blank(),
+   axis.text.y=element_blank())
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/ar_param-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process of order \emph{one} \emph{AR(1)} is defined by the formula: $r_i = \varphi_1 r_{i-1} + \varepsilon_i$
      \vskip1ex
      An \emph{AR(1)} process can be simulated recursively as follows:\\
      \hskip1em$r_1 = \varepsilon_1$\\
      \hskip1em$r_2 = \varphi_1 r_1 + \varepsilon_2=\varepsilon_2 + \varphi_1 \varepsilon_1$\\
      \hskip1em$r_3 = \varepsilon_3 + \varphi_1 \varepsilon_2 + \varphi_1^2 \varepsilon_1$\\
      \hskip1em$r_4 = \varepsilon_4 + \varphi_1 \varepsilon_3 + \varphi_1^2 \varepsilon_2 + \varphi_1^3 \varepsilon_1$
      \vskip1ex
      If $\varphi_1 < 1.0$ then the influence of any single shock $\varepsilon_i$ decays exponentially,
      \vskip1ex
      If $\varphi_1 = 1.0$ then the influence of any single shock $\varepsilon_i$ persists forever, and the variance of $r_i$ increases linearly with time,
      \vskip1ex
      An \emph{AR(1)} process has an exponentially declining ACF and a non-zero PACF at lag one,
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # simulate AR(1) process
> ari_ma <- arima.sim(n=729, model=list(ar=0.8))
> # ACF of AR(1) process
> acf_plus(ari_ma, lag=10, xlab="", ylab="",
+    main="ACF of AR(1) process")
> # PACF of AR(1) process
> pacf(ari_ma, lag=10, xlab="", ylab="",
+      main="PACF of AR(1) process")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/ar_acf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stationary Processes and Their Characteristic Equations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An autoregressive process is \emph{stationary} if its probability distribution does not change with time,
      \vskip1ex
      \emph{Stationary} processes have a constant mean and variance,
      \vskip1ex
      The \emph{autoregressive} process \emph{AR(p)}:
      $r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_{p} r_{i-p} + \varepsilon_i$
      \vskip1ex
      Has the following characteristic equation:
      $1 - \varphi_1 z - \varphi_2 z^2 - \ldots - \varphi_{p} z^p = 0$
      \vskip1ex
      An autoregressive process is stationary only if the absolute values of all the roots of its characteristic equation are greater than $1$,
      \vskip1ex
      An \emph{AR(1)} process:
      $r_i = \varphi_1 r_{i-1} + \varepsilon_i$
      has the following characteristic equation:
      $1 - \varphi_1 z = 0$,
      with a root equal to:
      $z = 1 / \varphi_1$,
      \vskip1ex
      If $\varphi_1 = 1$, then the characteristic equation has a \emph{unit root}:
      $z = 1 / \varphi_1$, and therefore isn't stationary,
      \vskip1ex
      The process follows:
      $r_i = r_{i-1} + \varepsilon_i$,
      \vskip1ex
      The above is called a \emph{Wiener} process, and it's a special case of a \emph{unit-root} process,
      \vskip1ex
      The variance of a \emph{Wiener} process is proportional to time,
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/stat_unit_root-1}
      \vspace{-4em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> rand_walk <- cumsum(zoo(matrix(rnorm(3*100), ncol=3),
+             order.by=(Sys.Date()+0:99)))
> colnames(rand_walk) <-
+   paste("rand_walk", 1:3, sep="_")
> plot(rand_walk, main="Random walks",
+      xlab="", ylab="", plot.type="single",
+      col=c("black", "red", "blue"))
> # add legend
> legend(x="topleft",
+  legend=colnames(rand_walk),
+  col=c("black", "red", "blue"), lty=1)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Integrated and Unit-root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset prices are the sum of simple asset returns, hence they follow an \emph{integrated} process with respect to asset returns:
      \begin{displaymath}
        x_i={\sum_{i=1}^{n}r_i}
      \end{displaymath}
      If returns follow an \emph{AR(1)} process:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varepsilon_i
      \end{displaymath}
      Then asset prices follow the process:
      \begin{displaymath}
        x_i = (1+\varphi_1) x_{i-1} - \varphi_1 x_{i-2} + \varepsilon_i
      \end{displaymath}
      If $\varphi_1=0$ then asset prices follow a \emph{Wiener} process (random walk),
      \vskip1ex
      A \emph{Wiener} process is a special case of a \emph{unit-root} process,
      \vskip1ex
      \emph{Unit-root} processes are not stationary, since their \emph{variance} isn't constant,
      \vskip1ex
      If $\varphi_1=0$ (no autocorrelation of returns) then asset prices follow a \emph{Wiener} process (random walk),
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/stat_unit_root-1}
      \vspace{-4em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> rand_walk <- cumsum(zoo(matrix(rnorm(3*100), ncol=3),
+             order.by=(Sys.Date()+0:99)))
> colnames(rand_walk) <-
+   paste("rand_walk", 1:3, sep="_")
> plot(rand_walk, main="Random walks",
+      xlab="", ylab="", plot.type="single",
+      col=c("black", "red", "blue"))
> # add legend
> legend(x="topleft",
+  legend=colnames(rand_walk),
+  col=c("black", "red", "blue"), lty=1)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Dickey-Fuller Test for Unit-roots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The \emph{Dickey-Fuller} and \emph{Augmented Dickey-Fuller} tests are designed to test the \emph{null hypothesis} that a time series process has a \emph{unit root},
      \vskip1ex
      The \emph{Augmented Dickey-Fuller} test fits the following regression model:
      \begin{displaymath}
        r_i = {\gamma}_1 x_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_{p} r_{i-p} + \varepsilon_i
      \end{displaymath}
      where $r_i = x_i - x_{i-1}$,
      \vskip1ex
      The \emph{null hypothesis} is that the process has a unit root ($\gamma = 0$), while the alternative hypothesis is that the process is stationary ($\gamma < 0$),
      \vskip1ex
      The \emph{ADF} test is weak in the sense that it needs a lot of data to identify a \emph{unit root} process,
    \column{0.6\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # simulate AR(1) process
> set.seed(1121)  # initialize random number generator
> ari_ma <- arima.sim(n=729, model=list(ar=0.8))
> adf.test(ari_ma)
> set.seed(1121)  # initialize random number generator
> ari_ma <- arima.sim(n=10000, model=list(ar=0.8))
> adf.test(ari_ma)
> set.seed(1121)  # initialize random number generator
> rand_walk <- cumsum(rnorm(729))
> adf.test(rand_walk)
> set.seed(1121)  # initialize random number generator
> rand_walk <- cumsum(rnorm(10000))
> adf.test(rand_walk)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Identification of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{AR}(3) process of order \emph{three} is defined by the formula:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \varphi_3 r_{i-3} + \varepsilon_i
      \end{displaymath}
      Autoregressive processes \emph{AR(p)} of order \emph{p} have an exponentially declining ACF and a non-zero PACF up to lag \emph{p},
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ar3_zoo <- zoo(  # AR(3) time series of returns
+   x=arima.sim(n=365,
+     model=list(ar=c(0.1, 0.5, 0.1))),
+   order.by=in_dex)  # zoo_arima
> # ACF of AR(3) process
> acf_plus(ar3_zoo, lag=10,
+  xlab="", ylab="", main="ACF of AR(3) process")
> 
> # PACF of AR(3) process
> pacf(ar3_zoo, lag=10,
+      xlab="", ylab="", main="PACF of AR(3) process")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/ar_pacf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fitting Autoregressive Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.3\textwidth}
      The function \texttt{arima()} from the base package \texttt{stats} fits a specified ARIMA model to a univariate time series,
      \vskip1ex
      The function \texttt{auto.arima()} from the package \texttt{forecast} automatically fits an ARIMA model to a univariate time series,
    \column{0.7\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ar3_zoo <- arima.sim(n=1000,
+       model=list(ar=c(0.1, 0.3, 0.1)))
> arima(ar3_zoo, order = c(5,0,0))  # fit AR(5) model
> library(forecast)  # load forecast
> auto.arima(ar3_zoo)  # fit ARIMA model
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Regression Analysis}


%%%%%%%%%%%%%%%
\subsection{Formula Objects}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      Formulas in \texttt{R} are defined using the "\textasciitilde{}" operator followed by a series of terms separated by the \texttt{"+"} operator,
      \vskip1ex
      Formulas can be defined as separate objects, manipulated, and passed to functions,
      \vskip1ex
      The formula "\texttt{z} \textasciitilde{} \texttt{x}" means the response variable \texttt{z} is explained by the explanatory variable \texttt{x},
      \vskip1ex
      The formula "\texttt{z \textasciitilde{} x + y}" represents a linear model: \texttt{z = ax  + by + c},
      \vskip1ex
      The formula "\texttt{z \textasciitilde{} x - 1}" or "\texttt{z \textasciitilde{} x + 0}" represents a linear model with zero intercept: $z = ax$,
      \vskip1ex
      The function \texttt{update()} modifies existing \texttt{formulas},
      \vskip1ex
      The \texttt{"."} symbol represents either all the remaining data, or the variable that was in this part of the formula,
    \column{0.6\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # formula of linear model with zero intercept
> lin_formula <- z ~ x + y - 1
> lin_formula
> 
> # collapsing a character vector into a text string
> paste0("x", 1:5)
> paste(paste0("x", 1:5), collapse="+")
> 
> # creating formula from text string
> lin_formula <- as.formula(  # coerce text strings to formula
+         paste("z ~ ",
+           paste(paste0("x", 1:5), collapse="+")
+           )  # end paste
+       )  # end as.formula
> class(lin_formula)
> lin_formula
> # modify the formula using "update"
> update(lin_formula, log(.) ~ . + beta)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simple Linear Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A Simple Linear Regression is a linear model between a response variable \texttt{z} and a single explanatory variable \texttt{x}, defined by the formula:
      \begin{displaymath}
        z_i = \alpha + \beta x_i + \varepsilon_i
      \end{displaymath}
      The data consists of $n$ observations of the response and explanatory variables, with the index $i$ ranging from $1$ to $n$,
      \vskip1ex
      $\alpha$ and $\beta$ are the unknown regression coefficients,
      \vskip1ex
      $\varepsilon_i$ are the residuals, assumed to be normally distributed, independent, and stationary,
      \vskip1ex
      In the Ordinary Least Squares method (OLS), the regression parameters are estimated by minimizing the sum of squared residuals, also called the residual sum of squares (RSS):
      \begin{align*}
        RSS = \sum_{i=1}^{n} {\varepsilon_i^2} = \sum_{i=1}^{n} {(z_i - \alpha - \beta x_i)^2}\\ = (z - \alpha \mathbb{1} - \beta x)^T (z - \alpha \mathbb{1} - \beta x)
      \end{align*}
      $\mathbb{1}$ is the unit vector, and $\mathbb{1}^T x = x^T \mathbb{1} = \sum_{i=1}^{n} {x_i}$
    \column{0.5\textwidth}
      The regression coefficients can be found by equating the RSS derivatives to zero:
      \begin{align*}
        RSS_\alpha = -2 (z - \alpha \mathbb{1} - \beta x)^T \mathbb{1} = 0\\
        RSS_\beta = -2 (z - \alpha \mathbb{1} - \beta x)^T x = 0
      \end{align*}
      The solution for $\alpha$ is:
      \begin{align*}
        \alpha = z^T \mathbb{1} - \beta x^T \mathbb{1}
      \end{align*}
      The solution for $\beta$ is:
      \begin{flalign*}
        & (z - (z^T \mathbb{1} - \beta x^T \mathbb{1}) \mathbb{1} - \beta x)^T x = 0\\
        & (z^T x - (z^T \mathbb{1} - \beta x^T \mathbb{1}) \mathbb{1}^T x - \beta x^T x) = 0\\
        & (z^T x - (z^T \mathbb{1}) (x^T \mathbb{1}) + \beta (x^T \mathbb{1})^2 - \beta x^T x) = 0\\
        & \beta = \frac {z^T x - (x^T \mathbb{1}) (z^T \mathbb{1}) } {x^T x - (\mathbb{1}^T x)^2}
      \end{flalign*}
      If the response and explanatory variables have zero mean, then $\alpha=0$ and $\beta=\frac {z^T x} {x^T x}$,
      \vskip1ex
      It's then easy to see that $\beta$ is proportional to the correlation coefficient between the response and explanatory variables,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Linear Regression with Multiple Explanatory Variables}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A Linear Regression model with $p$ explanatory variables $\{x_j\}$, is defined by the formula:
      \begin{displaymath}
        z_i = \alpha + \sum_{j=1}^{k} {\beta_j x_{i,j}} + \varepsilon_i
      \end{displaymath}
      Or in vector notation:
      \begin{displaymath}
        z = \alpha + \beta x + \varepsilon
      \end{displaymath}
      The response variable $z$ and the $p$ explanatory variables $\{x_j\}$ each contain $n$ observations,
      \vskip1ex
      The response variable $z$ is a vector of length $n$, and the explanatory variable $x$ is a $(n,p)$-dimensional matrix,
      \vskip1ex
      $\alpha$ and $\beta$ are the unknown regression coefficients, with $\alpha$ a scalar and $\beta$ a vector of length $p$,
      \vskip1ex
      $\varepsilon_i$ are the residuals, assumed to be normally distributed, independent, and stationary, with $\varepsilon$ a vector of length $p$,
    \column{0.5\textwidth}
      The OLS estimate for $\alpha$ is given by:
      \begin{align*}
        \alpha = z^T \mathbb{1} - \beta x^T \mathbb{1}
      \end{align*}
      If the variables are de-meaned, then the OLS estimate for $\beta$ is given by equating the RSS derivative to zero:
      \begin{flalign*}
        & RSS_\beta = - 2 (z - \beta x)^T x = 0\\
        & x^T z - \beta x^T x = 0\\
        & \beta = (x^T x)^{-1} x^T z
      \end{flalign*}
      The matrix $x^T x$ is the covariance matrix of the matrix $x$,
      \vskip1ex
      The covariance matrix $x^T x$ is invertible if the columns of $x$ are linearly independent,
      \vskip1ex
      The matrix $(x^T x)^{-1} x^T$ is known as the \emph{Moore-Penrose pseudoinverse} of the matrix $x$,
      \vskip1ex
      In the special case when the inverse matrix $x^{-1}$ does exist, then the \emph{pseudoinverse} matrix simplifies to the inverse: $(x^T x)^{-1} x^T = x^{-1} (x^T)^{-1} x^T = x^{-1}$
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Linear Regression Using \texttt{lm()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let the data generating process for the response variable be given as: $z = \alpha_{lat} + \beta_{lat} x + \varepsilon_{lat}$
      \vskip1ex
      Where $\alpha_{lat}$ and $\beta_{lat}$ are latent (unknown) coefficients, and $\varepsilon_{lat}$ is an unknown vector of random noise (error terms),
      \vskip1ex
      The error terms are the difference between the measured values of the response minus the (unknown) actual response values,
      \vskip1ex
      The function \texttt{lm()} fits a linear model into a set of data, and returns an object of class \texttt{"lm"}, which is a list containing the results of fitting the model:
      \begin{itemize}
        \item call - the model formula,
        \item coefficients - the fitted model coefficients ($\alpha$, $\beta_j$),
        \item residuals - the model residuals (response minus fitted values),
      \end{itemize}
      The regression residuals are not the same as the error terms, because the regression coefficients are not equal to the coefficients of the data generating process,
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # define explanatory variable
> explana_tory <- rnorm(100, mean=2)
> noise <- rnorm(100)
> # response equals linear form plus error terms
> res_ponse <- -3 + explana_tory + noise
> # specify regression formula
> reg_formula <- res_ponse ~ explana_tory
> reg_model <- lm(reg_formula)  # perform regression
> class(reg_model)  # regressions have class lm
> attributes(reg_model)
> eval(reg_model$call$formula)  # regression formula
> reg_model$coefficients  # regression coefficients
> coef(reg_model)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Regression Scatterplot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The generic function \texttt{plot()} produces a scatterplot when it's called on the regression formula,
      \vskip1ex
      \texttt{abline()} plots a straight line corresponding to the regression coefficients, when it's called on the regression object,
      \vskip1ex
      The fitted (predicted) values are the values of the response variable obtained from applying the regression model to the explanatory variables,
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(reg_formula)  # plot scatterplot using formula
> title(main="Simple Regression", line=-1)
> # add regression line
> abline(reg_model, lwd=2, col="red")
> # plot fitted (predicted) response values
> points(x=explana_tory, y=reg_model$fitted.values,
+        pch=16, col="blue")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/reg_scatter_plot.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Regression Summary}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{summary.lm()} produces a list of regression model summary statistics:
      \begin{itemize}
        \item coefficients: matrix with estimated coefficients, their \emph{t}-statistics, and \emph{p}-values,
        \item r.squared: fraction of response variance explained by the model (correlation between response and explanatory),
        \item adj.r.squared: r.squared adjusted for higher model complexity,
        \item fstatistic: ratio of variance explained by model divided by unexplained variance,
      \end{itemize}
      The regression \emph{null} hypothesis is that the regression coefficients are \emph{zero},
      \vskip1ex
      The \emph{t}-statistic (\emph{t}-value) is the ratio of the estimated value divided by its standard error,
      \vskip1ex
      The \emph{p}-value is the probability of obtaining the observed value of the \emph{t}-statistic, or even more extreme values, under the \emph{null} hypothesis,
      \vskip1ex
      A small \emph{p}-value is often interpreted as meaning that the coefficients are very unlikely to be zero (given the data),
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> reg_model_sum <- summary(reg_model)  # copy regression summary
> reg_model_sum  # print the summary to console
> attributes(reg_model_sum)$names  # get summary elements
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Interpreting the Regression Statistics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The regression \texttt{summary} is a list, and its elements can be accessed individually,
      \vskip1ex
      The standard errors of the regression are the standard deviations of the coefficients, given the residuals as the source of error,
      \vskip1ex
      The standard error of $\beta$ in a simple regression is given by: ${\sigma_\beta}^2 = \frac {1} {(n-2)} \frac {E[(\varepsilon^T x)^2]} {(x^T x)^2} = \frac {1} {(n-2)} \frac {E[\varepsilon^2]} {(x^T x)} = \frac {1} {(n-2)} \frac {{\sigma_\varepsilon}^2} {{\sigma_x}^2}$
      \vskip1ex
      The key assumption in the above formula for the standard error and the \emph{p}-value is that the residuals are normally distributed, independent, and stationary,
      \vskip1ex
      If the residuals are not normally distributed, independent, and stationary, then the standard error and the \emph{p}-value may be much bigger than reported by \texttt{summary.lm()}, and therefore the regression may not be statistically significant,
      \vskip1ex
      Market return time series are very far from normal, so the small \emph{p}-values shouldn't be automatically interpreted as meaning that the regression is statistically significant,
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> reg_model_sum$coefficients
> reg_model_sum$r.squared
> reg_model_sum$adj.r.squared
> reg_model_sum$fstatistic
> # standard error of beta
> reg_model_sum$
+   coefficients["explana_tory", "Std. Error"]
> sd(reg_model_sum$residuals)/sd(explana_tory)/
+   sqrt(unname(reg_model_sum$fstatistic[3]))
> anova(reg_model)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Weak Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the relationship between the response and explanatory variables is weak compared to the error terms (noise), then the regression will have low statistical significance,
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # high noise compared to coefficient
> res_ponse <- 3 + 2*explana_tory + rnorm(30, sd=8)
> reg_model <- lm(reg_formula)  # perform regression
> # estimate of regression coefficient is not
> # statistically significant
> summary(reg_model)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Influence of Noise on Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
    \vspace{-2em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> reg_stats <- function(std_dev) {  # noisy regression
+   set.seed(1121)  # initialize number generator
+ # create explanatory and response variables
+   explana_tory <- seq(from=0.1, to=3.0, by=0.1)
+   res_ponse <- 3 + 0.2*explana_tory +
+     rnorm(30, sd=std_dev)
+ # specify regression formula
+   reg_formula <- res_ponse ~ explana_tory
+ # perform regression and get summary
+   reg_model_sum <- summary(lm(reg_formula))
+ # extract regression statistics
+   with(reg_model_sum, c(pval=coefficients[2, 4],
+    adj_rsquared=adj.r.squared,
+    fstat=fstatistic[1]))
+ }  # end reg_stats
> # apply reg_stats() to vector of std dev values
> vec_sd <- seq(from=0.1, to=0.5, by=0.1)
> names(vec_sd) <- paste0("sd=", vec_sd)
> mat_stats <- t(sapply(vec_sd, reg_stats))
> # plot in loop
> par(mfrow=c(NCOL(mat_stats), 1))
> for (in_dex in 1:NCOL(mat_stats)) {
+   plot(mat_stats[, in_dex], type="l",
+  xaxt="n", xlab="", ylab="", main="")
+   title(main=colnames(mat_stats)[in_dex], line=-1.0)
+   axis(1, at=1:(NROW(mat_stats)),
+  labels=rownames(mat_stats))
+ }  # end for
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/reg_noise-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Influence of Noise on Regression Another Method}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
    \vspace{-2em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> reg_stats <- function(da_ta) {  # get regression
+ # perform regression and get summary
+   col_names <- colnames(da_ta)
+   reg_formula <-
+     paste(col_names[2], col_names[1], sep="~")
+   reg_model_sum <- summary(lm(reg_formula,
+                         data=da_ta))
+ # extract regression statistics
+   with(reg_model_sum, c(pval=coefficients[2, 4],
+    adj_rsquared=adj.r.squared,
+    fstat=fstatistic[1]))
+ }  # end reg_stats
> # apply reg_stats() to vector of std dev values
> vec_sd <- seq(from=0.1, to=0.5, by=0.1)
> names(vec_sd) <- paste0("sd=", vec_sd)
> mat_stats <-
+   t(sapply(vec_sd, function (std_dev) {
+     set.seed(1121)  # initialize number generator
+ # create explanatory and response variables
+     explana_tory <- seq(from=0.1, to=3.0, by=0.1)
+     res_ponse <- 3 + 0.2*explana_tory +
+ rnorm(30, sd=std_dev)
+     reg_stats(data.frame(explana_tory, res_ponse))
+     }))
> # plot in loop
> par(mfrow=c(NCOL(mat_stats), 1))
> for (in_dex in 1:NCOL(mat_stats)) {
+   plot(mat_stats[, in_dex], type="l",
+  xaxt="n", xlab="", ylab="", main="")
+   title(main=colnames(mat_stats)[in_dex], line=-1.0)
+   axis(1, at=1:(NROW(mat_stats)),
+  labels=rownames(mat_stats))
+ }  # end for
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/reg_noise-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Regression Diagnostic Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{plot()} produces diagnostic scatterplots for the residuals, when called on the regression object,
      \vskip1ex
      {\scriptsize
      The diagnostic scatterplots allow for visual inspection to determine the quality of the regression fit,
      \vskip1ex
      "Residuals vs Fitted" is a scatterplot of the residuals vs. the predicted responses,
      \vskip1ex
      "Scale-Location" is a scatterplot of the square root of the standardized residuals vs. the predicted responses,
      \vskip1ex
      The residuals should be randomly distributed around the horizontal line representing zero residual error,
      \vskip1ex
      A pattern in the residuals indicates that the model was not able to capture the relationship between the variables, or that the variables don't follow the statistical assumptions of the regression model,
      \vskip1ex
      "Normal Q-Q" is the standard Q-Q plot, and the points should fall on the diagonal line, indicating that the residuals are normally distributed,
      \vskip1ex
      "Residuals vs Leverage" is a scatterplot of the residuals vs. their leverage,
      \vskip1ex
      Leverage measures the amount by which the predicted response would change if the observed response were shifted by a small amount,
      \vskip1ex
      Cook's distance measures the influence of a single observation on the predicted values, and is proportional to the sum of the squared differences between predictions made with all observations and predictions made without the observation,
      \vskip1ex
      Points with large leverage, or a Cook's distance greater than 1 suggest the presence of an outlier or a poor model,
      }
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> par(mfrow=c(2, 2))  # plot 2x2 panels
> plot(reg_model)  # plot diagnostic scatterplots
> plot(reg_model, which=2)  # plot just Q-Q
\end{verbatim}
\end{kframe}
\end{knitrout}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/plot_reg-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Durbin-Watson Test of Autocorrelation of Residuals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Durbin-Watson} test is designed to test the \emph{null hypothesis} that the autocorrelations of regression residuals are equal to zero,
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        DW = \frac {\sum_{i=2}^{n} (\varepsilon_i - \varepsilon_{i-1})^2} {\sum_{i=1}^{n} \varepsilon_i^2}
      \end{displaymath}
      Where $\varepsilon_i$ are the regression residuals,
      \vskip1ex
      The value of the \emph{Durbin-Watson} statistic \emph{DW} is close to zero for large positive autocorrelations, and close to four for large negative autocorrelations,
      \vskip1ex
      The \emph{DW} is close to two for autocorrelations close to zero,
      \vskip1ex
      The \emph{p}-value for the \texttt{reg\_model} regression is large, and we conclude that the \emph{null hypothesis} is \texttt{TRUE}, and the regression residuals are uncorrelated,
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(lmtest)  # load lmtest
> # perform Durbin-Watson test
> dwtest(reg_model)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{devel: Autocorrelated Time Series Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filtering or smoothing a time series containing an error terms over overlapping periods introduces autocorrelations in the error terms of the time series,
      \vskip1ex
      Autocorrelations in the error terms introduces autocorrelations of the regression residuals, causing the Durbin-Watson test to fail,
      \vskip1ex
      Autocorrelations in the error terms introduce autocorrelations of the regression residuals, causing the Durbin-Watson test to fail,
      \vskip1ex
      The failure of the Durbin-Watson test means that the \emph{standard errors} and \emph{p}-values calculated by the regression model are too small, and therefore the regression may not be statistically significant,
      \vskip1ex
      But the failure of the Durbin-Watson test doesn't reject the existence of a linear relationship between the response and explanatory variables, it just puts it in doubt,
      \vskip1ex
      Links:
      https://onlinecourses.science.psu.edu/stat510/node/72
      http://stats.stackexchange.com/questions/6469/simple-linear-model-with-autocorrelated-errors-in-r
      \vskip1ex
      Regression of non-stationary time series creates \emph{spurious} regressions,
      \vskip1ex
      The \emph{t}-statistics, \emph{p}-values, and \emph{R}-squared all indicate a statistically significant regression,
      \vskip1ex
      But the Durbin-Watson test shows residuals are autocorrelated, which invalidates the other tests,
      \vskip1ex
      The Q-Q plot also shows that residuals are \emph{not} normally distributed,
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> tail(foo)
> class(foo)
> dim(foo)
> reg_model <- lm(paste(names(foo), collapse=" ~ "), data=foo)
> reg_model_sum <- summary(reg_model)
> reg_model_sum
> dwtest(reg_model)
> 
> # filter over non-overlapping periods
> bar <- names(foo)
> foo <- merge(period.sum(foo[, 1], INDEX=end_points), period.sum(foo[, 2], INDEX=end_points))
> foo <- foo[complete.cases(foo), ]
> names(foo) <- bar
> 
> # filter over overlapping periods
> foo <- rollsum(foo, k=11)
> 
> 
> set.seed(1121)
> library(lmtest)
> # spurious regression in unit root time series
> explana_tory <- cumsum(rnorm(100))  # unit root time series
> res_ponse <- cumsum(rnorm(100))
> reg_formula <- res_ponse ~ explana_tory
> reg_model <- lm(reg_formula)  # perform regression
> # summary indicates statistically significant regression
> reg_model_sum <- summary(reg_model)
> reg_model_sum$coefficients
> reg_model_sum$r.squared
> # Durbin-Watson test shows residuals are autocorrelated
> dw_test <- dwtest(reg_model)
> c(dw_test$statistic[[1]], dw_test$p.value)
\end{verbatim}
\end{kframe}
\end{knitrout}
      \vspace{-2em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(reg_formula, xlab="", ylab="")  # plot scatterplot using formula
> title(main="Spurious Regression", line=-1)
> # add regression line
> abline(reg_model, lwd=2, col="red")
> plot(reg_model, which=2, ask=FALSE)  # plot just Q-Q
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/autocorr_reg-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Omitted Variable Bias}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Omitted Variable Bias} occurs in a regression model that omits important predictors,
      \vskip1ex
      The parameter estimates are biased, even though the \emph{t}-statistics, \emph{p}-values, and \emph{R}-squared all indicate a statistically significant regression,
      \vskip1ex
      But the Durbin-Watson test shows residuals are autocorrelated, invalidating other tests,
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> design_matrix <- data.frame(  # design matrix
+   explana_tory=1:30, omit_var=sin(0.2*1:30))
> # response depends on both explanatory variables
> res_ponse <- with(design_matrix,
+   0.2*explana_tory + omit_var + 0.2*rnorm(30))
> # mis-specified regression only one explanatory
> reg_model <- lm(res_ponse ~ explana_tory,
+         data=design_matrix)
> reg_model_sum <- summary(reg_model)
> reg_model_sum$coefficients
> reg_model_sum$r.squared
> # Durbin-Watson test shows residuals are autocorrelated
> dwtest(reg_model)$p.value
\end{verbatim}
\end{kframe}
\end{knitrout}
      \vspace{-2em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(reg_formula, data=design_matrix)
> abline(reg_model, lwd=2, col="red")
> title(main="OVB Regression", line=-1)
> plot(reg_model, which=2, ask=FALSE)  # plot just Q-Q
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/ovb_reg-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Spurious Time Series Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Regression of non-stationary time series creates \emph{spurious} regressions,
      \vskip1ex
      The \emph{t}-statistics, \emph{p}-values, and \emph{R}-squared all indicate a statistically significant regression,
      \vskip1ex
      But the Durbin-Watson test shows residuals are autocorrelated, which invalidates the other tests,
      \vskip1ex
      The Q-Q plot also shows that residuals are \emph{not} normally distributed,
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> explana_tory <- cumsum(rnorm(100))  # unit root time series
> res_ponse <- cumsum(rnorm(100))
> reg_formula <- res_ponse ~ explana_tory
> reg_model <- lm(reg_formula)  # perform regression
> # summary indicates statistically significant regression
> reg_model_sum <- summary(reg_model)
> reg_model_sum$coefficients
> reg_model_sum$r.squared
> # Durbin-Watson test shows residuals are autocorrelated
> dw_test <- dwtest(reg_model)
> c(dw_test$statistic[[1]], dw_test$p.value)
\end{verbatim}
\end{kframe}
\end{knitrout}
      \vspace{-2em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(reg_formula, xlab="", ylab="")  # plot scatterplot using formula
> title(main="Spurious Regression", line=-1)
> # add regression line
> abline(reg_model, lwd=2, col="red")
> plot(reg_model, which=2, ask=FALSE)  # plot just Q-Q
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/spur_reg-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Predictions Using Regression Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{predict()} is a generic function for performing predictions based on a given model,
      \vskip1ex
      \texttt{predict.lm()} is the predict method for linear models (regressions),
      \vspace{-1em}
\begin{knitrout}\scriptsize
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> explana_tory <- seq(from=0.1, to=3.0, by=0.1)  # explanatory variable
> res_ponse <- 3 + 2*explana_tory + rnorm(30)
> reg_formula <- res_ponse ~ explana_tory
> reg_model <- lm(reg_formula)  # perform regression
> new_data <- data.frame(explana_tory=0.1*31:40)
> predict_lm <- predict(object=reg_model,
+               newdata=new_data, level=0.95,
+               interval="confidence")
> predict_lm <- as.data.frame(predict_lm)
> head(predict_lm, 2)
> plot(reg_formula, xlim=c(1.0, 4.0),
+      ylim=range(res_ponse, predict_lm),
+      main="Regression predictions")
> abline(reg_model, col="red")
> with(predict_lm, {
+   points(x=new_data$explana_tory, y=fit, pch=16, col="blue")
+   lines(x=new_data$explana_tory, y=lwr, lwd=2, col="red")
+   lines(x=new_data$explana_tory, y=upr, lwd=2, col="red")
+ })  # end with
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth,valign=t]{figure/predict_lm-1}
  \end{columns}
\end{block}

\end{frame}


\end{document}
